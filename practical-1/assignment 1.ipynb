{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bz2\n",
    "import codecs\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_file(file_name, selected_words = None):\n",
    "    word_index = []\n",
    "    embeddings = []\n",
    "\n",
    "    with bz2.BZ2File(file_name, \"r\") as reader:\n",
    "\n",
    "        for idx, line in enumerate(reader):\n",
    "            if idx % 10000 == 0:\n",
    "                print(\"Read {} lines\".format(idx))\n",
    "            line = line.strip().split()\n",
    "            word = line[0].decode('utf-8')\n",
    "            if selected_words is not None and word not in selected_words:\n",
    "                continue\n",
    "            embedding = np.array(list(map(float, line[1:])))\n",
    "            word_index.append(word)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "    return word_index, embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_index, embeddings = read_embedding_file(\"data/deps.words.bz2\");"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_index[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return round(np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSimilarityModel:\n",
    "    def __init__(self, word_index, embeddings, similarity=\"cosine\"):\n",
    "        self.word_index = dict(zip(word_index, np.arange(len(word_index))))\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        if similarity == \"cosine\":\n",
    "            self.similarity = cosine_similarity\n",
    "        else:\n",
    "            self.similarity = similarity\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        if word not in self.word_index:\n",
    "            raise ValueError(\"Word '{}' doesn't exist in the model\".format(word))\n",
    "        return self.embeddings[self.word_index[word]]\n",
    "\n",
    "    def most_similar(self, word, n=10, score=False):\n",
    "        u = self[word]\n",
    "        word_distances = []\n",
    "        for other_word, idx in self.word_index.items():\n",
    "            if word == other_word:\n",
    "                continue\n",
    "            word_distances.append((other_word, self.similarity(u, self[other_word])))\n",
    "        word_distances.sort(key=lambda _: -_[1])\n",
    "        if score:\n",
    "            return word_distances[:n]\n",
    "        else:\n",
    "            return list(map(lambda _: _[0], word_distances))[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    pkl_file = \"./models/{}.pkl\".format(model_name)\n",
    "    if not os.path.exists(pkl_file):\n",
    "        print(\"Model doesn't exist, creating it\")\n",
    "        word_index, embeddings = read_embedding_file(\"./data/{}.bz2\".format(model_name))\n",
    "        sim = WordSimilarityModel(word_index, embeddings)\n",
    "        pkl.dump(sim, open(pkl_file, \"wb\"))\n",
    "        return sim\n",
    "    return pkl.load(open(pkl_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dep_based = load_model(\"deps.words\")\n",
    "bow2_based = load_model(\"bow2.words\")\n",
    "bow5_based = load_model(\"bow5.words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [['Dependency ', dep_based], \n",
    "              ['BoW with k = 2 ', bow2_based], \n",
    "              ['BoW with k = 5 ', bow5_based]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Simlex999.txt \n",
    "\n",
    "def read_sim_file(filename):\n",
    "    pairs_list = []\n",
    "    pairs_dict = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        for _, line in enumerate(f):\n",
    "            line = line.strip().split()\n",
    "            pairs_list.append((line[0], line[1], line[3]))\n",
    "            pairs = pairs_list[1:]\n",
    "    for word1, word2, score in pairs:\n",
    "        if word1 not in pairs_dict.keys():\n",
    "            pairs_dict[word1] = []\n",
    "        pairs_dict[word1].append((word2,float(score)))\n",
    "# Sorting by scores\n",
    "    for word1, list_value in pairs_dict.items():\n",
    "        list_value.sort(key=lambda _: -_[1])\n",
    "    return pairs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read MEN-Dataset-Natural-Full-Form\n",
    "\n",
    "def read_men_file(filename):\n",
    "    pairs_list = []\n",
    "    pairs_dict = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        for _, line in enumerate(f):\n",
    "            line = line.strip().split()\n",
    "            pairs_list.append((line[0], line[1], line[2]))\n",
    "    for word1, word2, score in pairs_list:\n",
    "        if word1 not in pairs_dict.keys():\n",
    "            pairs_dict[word1] = []\n",
    "        pairs_dict[word1].append((word2, float(score)))\n",
    "    \n",
    "    for key, value in pairs_dict.items():\n",
    "        value.sort(key=lambda _: -_[1])\n",
    "        \n",
    "    return pairs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simlex_999 = read_sim_file(\"data/SimLex-999/SimLex-999.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "p[\"happy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_natural = read_men_file(\"data/MEN/MEN_dataset_natural_form_full\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "r[\"happy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models = [[\"SimLex-999\", simlex_999], \n",
    "               [\"MEN (Natural) Full Form\", men_natural]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "# For each common word from Simlex vs. models\n",
    "#     Find length of Simlex data\n",
    "#         Normalize scores of Simlex data in the range [0,1]\n",
    "#     Read similar length data in models\n",
    "#     Store scores in (a, b) - for spearman (two 1D arrays of the scores)\n",
    "spearman_a = np.zeros(len(p))\n",
    "spearman_b = np.zeros(len(p))\n",
    "i = 0\n",
    "for key, value in p.items():\n",
    "    for model_word in bow5_based.word_index.keys():\n",
    "        if (key == model_word):\n",
    "            if i%50 == 0:\n",
    "                print(i)\n",
    "            simlex_length = len(value)\n",
    "            model_all_values = bow5_based.most_similar(key, score = True, n = simlex_length)\n",
    "            normalized_sum = 0.0\n",
    "\n",
    "            for _, score in value:\n",
    "                normalized_sum += score\n",
    "            for _, score in value:\n",
    "                score = round(score/normalized_sum, 3)\n",
    "                spearman_a[i] = score\n",
    "            for _, model_score in model_all_values:\n",
    "                spearman_b[i] = (model_score)\n",
    "        else:\n",
    "            print(key, \" doesn't exist in the model.\\n\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.3767945615424258, pvalue=3.2510867767480334e-22)\n"
     ]
    }
   ],
   "source": [
    "print(stats.spearmanr(spearman_a, spearman_b, axis = None))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.497, 0.512, 0.469, 0.579, 0.691, 0.605, 0.812, 0.645, 0.665,\n",
       "       0.522, 0.589, 0.576, 0.736, 0.526, 0.559, 0.542, 0.588, 0.67 ,\n",
       "       0.696, 0.702, 0.748, 0.64 , 0.722, 0.582, 0.584, 0.688, 0.769,\n",
       "       0.642, 0.735, 0.492, 0.672, 0.828, 0.485, 0.729, 0.785, 0.664,\n",
       "       0.633, 0.68 , 0.762, 0.727, 0.519, 0.796, 0.703, 0.765, 0.732,\n",
       "       0.559, 0.607, 0.605, 0.542, 0.739, 0.715, 0.595, 0.702, 0.594,\n",
       "       0.703, 0.801, 0.541, 0.561, 0.698, 0.673, 0.711, 0.584, 0.683,\n",
       "       0.74 , 0.659, 0.733, 0.772, 0.549, 0.666, 0.55 , 0.576, 0.618,\n",
       "       0.622, 0.758, 0.79 , 0.548, 0.7  , 0.661, 0.848, 0.599, 0.845,\n",
       "       0.614, 0.568, 0.714, 0.551, 0.58 , 0.688, 0.593, 0.853, 0.668,\n",
       "       0.825, 0.6  , 0.624, 0.604, 0.735, 0.552, 0.725, 0.707, 0.546,\n",
       "       0.609, 0.657, 0.638, 0.75 , 0.772, 0.542, 0.726, 0.819, 0.576,\n",
       "       0.805, 0.635, 0.675, 0.721, 0.649, 0.721, 0.752, 0.592, 0.573,\n",
       "       0.868, 0.58 , 0.624, 0.713, 0.758, 0.502, 0.586, 0.612, 0.583,\n",
       "       0.66 , 0.717, 0.564, 0.734, 0.826, 0.661, 0.691, 0.648, 0.632,\n",
       "       0.61 , 0.64 , 0.736, 0.637, 0.758, 0.585, 0.684, 0.723, 0.727,\n",
       "       0.575, 0.715, 0.768, 0.549, 0.702, 0.561, 0.705, 0.691, 0.636,\n",
       "       0.549, 0.691, 0.722, 0.672, 0.694, 0.666, 0.797, 0.565, 0.685,\n",
       "       0.709, 0.752, 0.573, 0.727, 0.76 , 0.78 , 0.662, 0.482, 0.519,\n",
       "       0.635, 0.682, 0.703, 0.679, 0.517, 0.701, 0.754, 0.799, 0.599,\n",
       "       0.788, 0.795, 0.735, 0.626, 0.718, 0.747, 0.704, 0.591, 0.688,\n",
       "       0.696, 0.532, 0.598, 0.65 , 0.802, 0.633, 0.834, 0.69 , 0.58 ,\n",
       "       0.585, 0.627, 0.694, 0.733, 0.773, 0.715, 0.655, 0.658, 0.764,\n",
       "       0.556, 0.607, 0.626, 0.602, 0.708, 0.663, 0.777, 0.762, 0.793,\n",
       "       0.506, 0.587, 0.607, 0.605, 0.708, 0.676, 0.586, 0.688, 0.697,\n",
       "       0.779, 0.608, 0.702, 0.862, 0.695, 0.786, 0.768, 0.559, 0.843,\n",
       "       0.747, 0.685, 0.72 , 0.715, 0.842, 0.605, 0.629, 0.491, 0.836,\n",
       "       0.593, 0.75 , 0.622, 0.684, 0.666, 0.615, 0.79 , 0.72 , 0.547,\n",
       "       0.754, 0.743, 0.809, 0.624, 0.634, 0.564, 0.706, 0.765, 0.694,\n",
       "       0.781, 0.849, 0.563, 0.7  , 0.594, 0.698, 0.973, 0.781, 0.705,\n",
       "       0.613, 0.735, 0.845, 0.705, 0.681, 0.72 , 0.559, 0.565, 0.87 ,\n",
       "       0.637, 0.796, 0.588, 0.646, 0.457, 0.648, 0.794, 0.815, 0.754,\n",
       "       0.804, 0.6  , 0.698, 0.534, 0.636, 0.776, 0.874, 0.848, 0.583,\n",
       "       0.754, 0.729, 0.697, 0.816, 0.637, 0.639, 0.693, 0.675, 0.707,\n",
       "       0.816, 0.8  , 0.654, 0.648, 0.578, 0.759, 0.709, 0.663, 0.745,\n",
       "       0.677, 0.687, 0.709, 0.635, 0.526, 0.807, 0.483, 0.807, 0.684,\n",
       "       0.668, 0.807, 0.773, 0.746, 0.595, 0.64 , 0.72 , 0.81 , 0.526,\n",
       "       0.681, 0.512, 0.789, 0.611, 0.486, 0.62 , 0.627, 0.677, 0.692,\n",
       "       0.715, 0.82 , 0.634, 0.734, 0.81 , 0.635, 0.802, 0.82 , 0.631,\n",
       "       0.692, 0.686, 0.627, 0.581, 0.635, 0.655, 0.687, 0.823, 0.828,\n",
       "       0.75 , 0.93 , 0.791, 0.615, 0.793, 0.717, 0.757, 0.696, 0.674,\n",
       "       0.533, 0.66 , 0.624, 0.483, 0.596, 0.773, 0.477, 0.728, 0.677,\n",
       "       0.673, 0.668, 0.579, 0.51 , 0.625, 0.775, 0.661, 0.749, 0.675,\n",
       "       0.589, 0.554, 0.812, 0.818, 0.853, 0.718, 0.703, 0.667, 0.667,\n",
       "       0.754, 0.648, 0.716, 0.779, 0.707, 0.589, 0.636, 0.616, 0.812,\n",
       "       0.688, 0.535, 0.623, 0.545, 0.655, 0.697, 0.802, 0.759, 0.669,\n",
       "       0.677, 0.505, 0.787, 0.617, 0.479, 0.762, 0.829, 0.658, 0.748,\n",
       "       0.702, 0.747, 0.813, 0.735, 0.748, 0.89 , 0.665, 0.854, 0.712,\n",
       "       0.739, 0.854, 0.626, 0.647, 0.74 , 0.795, 0.667, 0.728, 0.63 ,\n",
       "       0.714, 0.634, 0.713, 0.844, 0.711, 0.804, 0.761, 0.714, 0.652,\n",
       "       0.769, 0.517, 0.709, 0.668, 0.658, 0.689, 0.834, 0.806, 0.632,\n",
       "       0.695, 0.53 , 0.74 , 0.738, 0.595, 0.78 , 0.587, 0.746, 0.693,\n",
       "       0.631, 0.66 , 0.804, 0.611, 0.549, 0.848, 0.642, 0.54 , 0.66 ,\n",
       "       0.668, 0.537, 0.601, 0.752, 0.624, 0.799, 0.56 , 0.82 , 0.671,\n",
       "       0.872, 0.701, 0.764, 0.68 , 0.788, 0.674, 0.587, 0.489, 0.677,\n",
       "       0.748, 0.755, 0.683, 0.665, 0.812, 0.652, 0.756, 0.687, 0.643,\n",
       "       0.629, 0.618, 0.696, 0.554, 0.649, 0.614, 0.757, 0.7  , 0.608,\n",
       "       0.748, 0.675, 0.591, 0.637, 0.749, 0.642, 0.675, 0.667, 0.651,\n",
       "       0.727, 0.633, 0.578, 0.69 , 0.644, 0.554, 0.682, 0.563, 0.706,\n",
       "       0.612, 0.693, 0.704, 0.653, 0.647, 0.616, 0.671, 0.661, 0.617,\n",
       "       0.759, 0.723, 0.701, 0.642, 0.674, 0.775, 0.68 , 0.688, 0.   ,\n",
       "       0.645, 0.768, 0.602, 0.746, 0.613, 0.701, 0.678, 0.644, 0.724,\n",
       "       0.581, 0.632, 0.74 , 0.737, 0.706, 0.728, 0.742, 0.652, 0.71 ,\n",
       "       0.657, 0.64 , 0.701, 0.637, 0.786, 0.616, 0.719, 0.697, 0.775,\n",
       "       0.677, 0.725, 0.702, 0.928, 0.742, 0.699, 0.681, 0.682, 0.683,\n",
       "       0.672, 0.692, 0.771, 0.735, 0.608, 0.618, 0.875, 0.683, 0.668,\n",
       "       0.643, 0.758, 0.603, 0.726, 0.586, 0.788, 0.597, 0.823, 0.649,\n",
       "       0.628, 0.685, 0.704, 0.675, 0.622, 0.609, 0.734, 0.686, 0.807,\n",
       "       0.652, 0.86 , 0.728, 0.665])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
