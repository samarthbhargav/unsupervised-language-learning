{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_analogy_data(path):\n",
    "    with open(path) as reader:\n",
    "        analogy_data = []\n",
    "        task_labels = []\n",
    "        for line in reader:\n",
    "            if line.startswith(\":\"):\n",
    "                task = line.strip().strip(\":\").strip()\n",
    "                continue\n",
    "            # convert to lower-case \n",
    "            analogy_data.append(line.strip().lower().split())\n",
    "            task_labels.append(task)\n",
    "    return analogy_data, task_labels\n",
    "analogy_data, task_labels = read_analogy_data(\"./data/questions-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['athens', 'greece', 'baghdad', 'iraq'],\n",
       "  ['athens', 'greece', 'bangkok', 'thailand'],\n",
       "  ['athens', 'greece', 'beijing', 'china'],\n",
       "  ['athens', 'greece', 'berlin', 'germany'],\n",
       "  ['athens', 'greece', 'bern', 'switzerland'],\n",
       "  ['athens', 'greece', 'cairo', 'egypt'],\n",
       "  ['athens', 'greece', 'canberra', 'australia'],\n",
       "  ['athens', 'greece', 'hanoi', 'vietnam'],\n",
       "  ['athens', 'greece', 'havana', 'cuba'],\n",
       "  ['athens', 'greece', 'helsinki', 'finland']],\n",
       " ['capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of sub-categories\n",
    "analogy_data[:10], task_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'capital-common-countries': 506,\n",
       "         'capital-world': 4524,\n",
       "         'city-in-state': 2467,\n",
       "         'currency': 866,\n",
       "         'family': 506,\n",
       "         'gram1-adjective-to-adverb': 992,\n",
       "         'gram2-opposite': 812,\n",
       "         'gram3-comparative': 1332,\n",
       "         'gram4-superlative': 1122,\n",
       "         'gram5-present-participle': 1056,\n",
       "         'gram6-nationality-adjective': 1599,\n",
       "         'gram7-past-tense': 1560,\n",
       "         'gram8-plural': 1332,\n",
       "         'gram9-plural-verbs': 870})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2_sim = load_model(\"bow2.words\")\n",
    "#bow5_sim = load_model(\"bow5.words\")\n",
    "#deps_sim = load_model(\"deps.words\")\n",
    "\n",
    "# models = {\n",
    "#     \"bow2\": bow2_sim,\n",
    "#     \"bow5\": bow5_sim,\n",
    "#     \"deps\": deps_sim\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank(correct_value, results):\n",
    "    try:\n",
    "        position = results.index(correct_value)\n",
    "        return 1 / (position + 1)\n",
    "    except:\n",
    "        return 0 \n",
    "\n",
    "# tests\n",
    "print(reciprocal_rank(\"cats\", [\"catten\", \"cati\", \"cats\"]))\n",
    "print(reciprocal_rank(\"tori\", [\"catten\", \"tori\", \"cats\"]))\n",
    "print(reciprocal_rank(\"virus\", [\"virus\", \"cati\", \"cats\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wv(model, a, a_star, b):\n",
    "    if a not in model.word_index or a_star not in model.word_index or b not in model.word_index:\n",
    "        return None\n",
    "    a, a_star, b = model[a], model[a_star], model[b]\n",
    "    v = a_star - a\n",
    "    b_star = b + v\n",
    "    return b_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'capital-common-countries'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import threading\n",
    "\n",
    "# class AnalogyThread(threading.Thread):\n",
    "#     def __init__(self, task, model, data):\n",
    "#         threading.Thread.__init__(self)\n",
    "#         self.task = task\n",
    "#         self.model = model\n",
    "#         self.data = data\n",
    "#         self.task_rr = None\n",
    "#         self.task_correct = None\n",
    "    \n",
    "#     def run(self):\n",
    "#         print(\"Running: {}\".format(self.task))\n",
    "#         self.task_rr, self.task_correct = evaluate_model(bow2_sim, analogy_data, self.task)\n",
    "#         print(\"Finished: {}\".format(self.task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threads = []\n",
    "# for task in np.unique(task_labels):\n",
    "#     at = AnalogyThread(task, bow2_sim, analogy_data)\n",
    "#     at.start()\n",
    "#     threads.append(at)\n",
    "    \n",
    "# for thread in threads:\n",
    "#     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, data, task):\n",
    "#     task_correct = defaultdict(list)\n",
    "#     task_rr = defaultdict(list)\n",
    "\n",
    "#     for index, (tlab, (a, a_star, b, b_star_actual)) in enumerate(zip(task_labels, data)):\n",
    "#         if tlab != task:\n",
    "#             continue\n",
    "            \n",
    "#         if b_star_actual not in model.word_index:\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "#         b_star = compute_wv(model, a, a_star, b)\n",
    "\n",
    "#         if b_star is None:\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "\n",
    "#         results = model.most_similar_to_vector(b_star, n=len(model.word_index), score=False)\n",
    "#         # exclude these\n",
    "#         results = [r for r in results if r not in {a, a_star, b}]\n",
    "        \n",
    "#         if results[0] == b_star_actual:\n",
    "#             task_correct[tlab].append(1)\n",
    "#         else:\n",
    "#             task_correct[tlab].append(0)\n",
    "        \n",
    "#         task_rr[tlab].append(reciprocal_rank(b_star_actual, results))\n",
    "        \n",
    "#         if index == 2:\n",
    "#             break\n",
    "        \n",
    "#         if index % 100 == 0:\n",
    "#             print(\"Task: {}, {}\".format(index))\n",
    "    \n",
    "#     accuracy = sum(task_correct[task]) / len(task_correct[task])\n",
    "#     print(\"\\t{}:: Accuracy: {}, MRR: {}\".format(task, accuracy, np.mean(task_rr[task])))\n",
    "    \n",
    "#     return task_correct, task_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data):\n",
    "    overall_correct = []\n",
    "    overall_rr = []\n",
    "    task_correct = defaultdict(list)\n",
    "    task_rr = defaultdict(list)\n",
    "\n",
    "    for index, (tlab, (a, a_star, b, b_star_actual)) in enumerate(zip(task_labels, data)):\n",
    "        if b_star_actual not in model.word_index:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        b_star = compute_wv(model, a, a_star, b)\n",
    "\n",
    "        if b_star is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        results = model.most_similar_to_vector(b_star, n=len(model.word_index), score=False)\n",
    "        # exclude these\n",
    "        results = [r for r in results if r not in {a, a_star, b}]\n",
    "        \n",
    "        if results[0] == b_star_actual:\n",
    "            overall_correct.append(1)\n",
    "            task_correct[tlab].append(1)\n",
    "        else:\n",
    "            overall_correct.append(0)\n",
    "            task_correct[tlab].append(0)\n",
    "        \n",
    "        overall_rr.append(reciprocal_rank(b_star_actual, results))\n",
    "        task_rr[tlab].append(reciprocal_rank(b_star_actual, results))\n",
    "\n",
    "        if index % 100 == 0:\n",
    "            print(index)\n",
    "    \n",
    "    accuracy = sum(overall_correct) / len(overall_correct)\n",
    "    print(\"Accuracy: {}, MRR: {}\".format(accuracy, np.mean(overall_rr)))\n",
    "    \n",
    "    print(\"Per task: \")\n",
    "    for task in np.unique(task_labels):\n",
    "        accuracy = sum(task_correct[task]) / len(task_correct[task])\n",
    "        print(\"\\t{}:: Accuracy: {}, MRR: {}\".format(task, accuracy, np.mean(task_rr[task])))\n",
    "    \n",
    "evaluate_model(bow2_sim, analogy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def compute_scores(data, model, task_labels):\n",
    "#     skipped = 0\n",
    "    \n",
    "#     overall_correct = []\n",
    "#     overall_reciprocal_ranks = []\n",
    "    \n",
    "#     for task_label in np.unique(task_labels):\n",
    "#         print(\"Task: \", task_label)\n",
    "#         word_vectors = []\n",
    "#         parsed_data = []\n",
    "        \n",
    "#         for index, (tlab, (a, a_star, b, b_star_actual)) in enumerate(zip(task_labels, data)):\n",
    "#             if tlab != task_label:\n",
    "#                 continue\n",
    "            \n",
    "#             # if the final word doesn't exist, then there's no point\n",
    "#             if b_star_actual not in model.word_index:\n",
    "#                 skipped += 1\n",
    "#                 continue\n",
    "\n",
    "#             if index % 250 == 0:\n",
    "#                 print(\"\\tPreprocessing: {} of {}\".format(index, len(data)))\n",
    "\n",
    "#             b_star = compute_wv(model, a, a_star, b)\n",
    "#             if b_star is None:\n",
    "#                 skipped += 1\n",
    "#                 continue\n",
    "\n",
    "#             parsed_data.append((a, a_star, b, b_star_actual))\n",
    "#             word_vectors.append(b_star)\n",
    "\n",
    "#         word_vectors = np.array(word_vectors)\n",
    "#         print(\"\\t\\t ... done. Constructed word matrix of size: {}\".format(word_vectors.shape))\n",
    "#         print(\"\\tNow computing similarities!\")\n",
    "    \n",
    "#         similarity_dict = defaultdict(list)\n",
    "#         for index, (word, word_index) in enumerate(model.word_index.items()):\n",
    "#             if index % 100 == 0:\n",
    "#                 print(\"\\tSimilarity : {} of {}\".format(index, len(model.word_index)))\n",
    "#             word_vector = model.embeddings[word_index]\n",
    "\n",
    "#             similarities = np.apply_along_axis(lambda _: cosine_similarity(word_vector, _), 1, word_vectors)\n",
    "\n",
    "#             for idx, tup in enumerate(parsed_data):\n",
    "#                 similarity_dict[tuple(tup)].append((word, similarities[idx]))\n",
    "\n",
    "#         correct = defaultdict(list)\n",
    "#         reciprocal_ranks = defaultdict(list)\n",
    "\n",
    "#         for tup, task in zip(data, task_labels):\n",
    "#             if task != task_label:\n",
    "#                 continue\n",
    "#             tup = tuple(tup)\n",
    "#             if tup not in similarity_dict:\n",
    "#                 continue\n",
    "#             similarity_list = similarity_dict[tup]\n",
    "#             similarity_list.sort(key=lambda _: -_[1])\n",
    "#             if similarity_list[0] == b_star_actual:\n",
    "#                 correct[task].append(1)\n",
    "#             else:\n",
    "#                 correct[task].append(0)\n",
    "#             b_star_results = [_[0] for _ in similarity_list]\n",
    "#             reciprocal_ranks[task].append(reciprocal_rank(b_star_actual, b_star_results))\n",
    "\n",
    "#         overall_correct.extend(correct[task])\n",
    "#         overall_reciprocal_ranks.extend(reciprocal_ranks[task])\n",
    "        \n",
    "#         accuracy = np.sum(correct[task]) / len(correct[task])\n",
    "#         mrr = np.mean(reciprocal_ranks[task])\n",
    "#         print(\"\\tTask: {} :: Accuracy: {}, MRR: {}\".format(task, accuracy, mrr))        \n",
    "        \n",
    "#     overall_acc = np.sum(overall_correct) / len(overall_correct) \n",
    "#     print(\"Overall: Accuracy: {}, MRR: {}\".format(overall_acc, np.mean(overall_reciprocal_ranks)))\n",
    "    \n",
    "#     print(\"\\t ... done\")\n",
    "#     print(\"\\t Skipped: {}\".format(skipped))\n",
    "\n",
    "# compute_scores(analogy_data, bow2_sim, task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
