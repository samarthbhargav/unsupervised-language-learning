{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_analogy_data(path):\n",
    "    with open(path) as reader:\n",
    "        analogy_data = defaultdict(list)\n",
    "        for line in reader:\n",
    "            if line.startswith(\":\"):\n",
    "                task = line.strip().strip(\":\").strip()\n",
    "                continue\n",
    "            # convert to lower-case \n",
    "            analogy_data[task].append(line.strip().lower().split())\n",
    "    return analogy_data\n",
    "analogy_data = read_analogy_data(\"./data/questions-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gram7-past-tense', 'gram8-plural', 'city-in-state', 'currency', 'gram2-opposite', 'gram6-nationality-adjective', 'gram9-plural-verbs', 'gram1-adjective-to-adverb', 'family', 'capital-world', 'capital-common-countries', 'gram4-superlative', 'gram5-present-participle', 'gram3-comparative'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of sub-categories\n",
    "analogy_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gram7-past-tense 1560\n",
      "gram8-plural 1332\n",
      "city-in-state 2467\n",
      "currency 866\n",
      "gram2-opposite 812\n",
      "gram6-nationality-adjective 1599\n",
      "gram9-plural-verbs 870\n",
      "gram1-adjective-to-adverb 992\n",
      "family 506\n",
      "capital-world 4524\n",
      "capital-common-countries 506\n",
      "gram4-superlative 1122\n",
      "gram5-present-participle 1056\n",
      "gram3-comparative 1332\n"
     ]
    }
   ],
   "source": [
    "for key in analogy_data.keys():\n",
    "    print(key, len(analogy_data[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model doesn't exist, creating it\n",
      "Read 0 lines\n",
      "Read 10000 lines\n",
      "Read 20000 lines\n",
      "Read 30000 lines\n",
      "Read 40000 lines\n",
      "Read 50000 lines\n",
      "Read 60000 lines\n",
      "Read 70000 lines\n",
      "Read 80000 lines\n",
      "Read 90000 lines\n",
      "Read 100000 lines\n",
      "Read 110000 lines\n",
      "Read 120000 lines\n",
      "Read 130000 lines\n",
      "Read 140000 lines\n",
      "Read 150000 lines\n",
      "Read 160000 lines\n",
      "Read 170000 lines\n",
      "Read 180000 lines\n"
     ]
    }
   ],
   "source": [
    "bow2_sim = load_model(\"bow2.words\")\n",
    "#bow5_sim = load_model(\"bow5.words\")\n",
    "#deps_sim = load_model(\"deps.words\")\n",
    "\n",
    "# models = {\n",
    "#     \"bow2\": bow2_sim,\n",
    "#     \"bow5\": bow5_sim,\n",
    "#     \"deps\": deps_sim\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank(correct_value, results):\n",
    "    position = results.index(correct_value)\n",
    "    if position == -1:\n",
    "        return 0 \n",
    "    return 1 / (position + 1)\n",
    "\n",
    "# tests\n",
    "# print(reciprocal_rank(\"cats\", [\"catten\", \"cati\", \"cats\"]))\n",
    "# print(reciprocal_rank(\"tori\", [\"catten\", \"tori\", \"cats\"]))\n",
    "# print(reciprocal_rank(\"virus\", [\"virus\", \"cati\", \"cats\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores for task:  gram7-past-tense\n",
      "\t0 of 1560\n",
      "\t100 of 1560\n",
      "\t200 of 1560\n",
      "\t300 of 1560\n",
      "\t400 of 1560\n",
      "\t500 of 1560\n",
      "\t600 of 1560\n",
      "\t700 of 1560\n",
      "\t800 of 1560\n",
      "\t900 of 1560\n",
      "\t1000 of 1560\n"
     ]
    }
   ],
   "source": [
    "def compute_analogy(model, a, a_star, b):\n",
    "    if a not in model.word_index or a_star not in model.word_index or b not in model.word_index:\n",
    "        return None\n",
    "    a, a_star, b = model[a], model[a_star], model[b]\n",
    "    v = a_star - a\n",
    "    b_star = b + v\n",
    "    return model.most_similar_to_vector(b_star, n=len(model.embeddings))\n",
    "\n",
    "def compute_scores(task, model):\n",
    "    print(\"Computing scores for task: \", task)\n",
    "    data = analogy_data[task]\n",
    "    \n",
    "    correct = 0\n",
    "    reciprocal_ranks = []\n",
    "    skipped = 0\n",
    "    for index, (a, a_star, b, b_star_actual) in enumerate(data):\n",
    "        # if the final word doesn't exist, then there's no point\n",
    "        if b_star_actual not in model.word_index:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if index % 100 == 0:\n",
    "            print(\"\\t{} of {}\".format(index, len(data)))\n",
    "        b_star_results = compute_analogy(model, a, a_star, b)\n",
    "        \n",
    "        # ignore if some of the words are not in the vocabulary\n",
    "        if b_star_results is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        if b_star_results[0] == b_star_actual:\n",
    "            correct += 1\n",
    "        \n",
    "        reciprocal_ranks.append(reciprocal_rank(b_star_actual, b_star_results))\n",
    "        \n",
    "    print(\"\\t ... done\")\n",
    "    accuracy, mrr = correct / len(data), np.mean(reciprocal_ranks)\n",
    "    print(\"\\t Skipped: {}, Accuracy: {}, MRR: {}\".format(skipped, accuracy, mrr))\n",
    "    return accuracy, mrr, correct, reciprocal_ranks\n",
    "\n",
    "\n",
    "def run_evaluation(model):    \n",
    "    scores = defaultdict(dict)\n",
    "    overall_correct = 0\n",
    "    overall_evaluated = 0\n",
    "    overall_reciprocal_ranks = []\n",
    "    \n",
    "    for task, data in analogy_data.items():\n",
    "        accuracy, mrr, correct, reciprocal_ranks = compute_scores(task, model)\n",
    "        scores[task][\"accuracy\"] = accuracy\n",
    "        scores[task][\"MRR\"] = mrr\n",
    "        \n",
    "        overall_correct += correct\n",
    "        overall_evaluated += len(reciprocal_ranks)\n",
    "        overall_reciprocal_ranks.extend(reciprocal_ranks)\n",
    "    \n",
    "    scores[\"overall\"][\"accuracy\"] = overall_correct / overall_evaluated\n",
    "    scores[\"overall\"][\"MRR\"] = np.mean(overall_reciprocal_ranks)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "bow2_scores = run_evaluation(bow2_sim)\n",
    "pickle.dump(bow2_scores, open(\"bow2_analogy_scores.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
