{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_analogy_data(path):\n",
    "    with open(path) as reader:\n",
    "        analogy_data = []\n",
    "        task_labels = []\n",
    "        for line in reader:\n",
    "            if line.startswith(\":\"):\n",
    "                task = line.strip().strip(\":\").strip()\n",
    "                continue\n",
    "            # convert to lower-case \n",
    "            analogy_data.append(line.strip().lower().split())\n",
    "            task_labels.append(task)\n",
    "    return analogy_data, task_labels\n",
    "analogy_data, task_labels = read_analogy_data(\"./data/questions-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['athens', 'greece', 'baghdad', 'iraq'],\n",
       "  ['athens', 'greece', 'bangkok', 'thailand'],\n",
       "  ['athens', 'greece', 'beijing', 'china'],\n",
       "  ['athens', 'greece', 'berlin', 'germany'],\n",
       "  ['athens', 'greece', 'bern', 'switzerland'],\n",
       "  ['athens', 'greece', 'cairo', 'egypt'],\n",
       "  ['athens', 'greece', 'canberra', 'australia'],\n",
       "  ['athens', 'greece', 'hanoi', 'vietnam'],\n",
       "  ['athens', 'greece', 'havana', 'cuba'],\n",
       "  ['athens', 'greece', 'helsinki', 'finland']],\n",
       " ['capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries',\n",
       "  'capital-common-countries'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of sub-categories\n",
    "analogy_data[:10], task_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'capital-common-countries': 506,\n",
       "         'capital-world': 4524,\n",
       "         'city-in-state': 2467,\n",
       "         'currency': 866,\n",
       "         'family': 506,\n",
       "         'gram1-adjective-to-adverb': 992,\n",
       "         'gram2-opposite': 812,\n",
       "         'gram3-comparative': 1332,\n",
       "         'gram4-superlative': 1122,\n",
       "         'gram5-present-participle': 1056,\n",
       "         'gram6-nationality-adjective': 1599,\n",
       "         'gram7-past-tense': 1560,\n",
       "         'gram8-plural': 1332,\n",
       "         'gram9-plural-verbs': 870})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2_sim = load_model(\"bow2.words\")\n",
    "#bow5_sim = load_model(\"bow5.words\")\n",
    "#deps_sim = load_model(\"deps.words\")\n",
    "\n",
    "# models = {\n",
    "#     \"bow2\": bow2_sim,\n",
    "#     \"bow5\": bow5_sim,\n",
    "#     \"deps\": deps_sim\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(correct_value, results):\n",
    "    try:\n",
    "        position = results.index(correct_value)\n",
    "        return 1 / (position + 1)\n",
    "    except:\n",
    "        return 0 \n",
    "\n",
    "# tests\n",
    "# print(reciprocal_rank(\"cats\", [\"catten\", \"cati\", \"cats\"]))\n",
    "# print(reciprocal_rank(\"tori\", [\"catten\", \"tori\", \"cats\"]))\n",
    "# print(reciprocal_rank(\"virus\", [\"virus\", \"cati\", \"cats\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPreprocessing: 0 of 19544\n",
      "\tPreprocessing: 250 of 19544\n",
      "\tPreprocessing: 500 of 19544\n",
      "\tPreprocessing: 750 of 19544\n",
      "\tPreprocessing: 1000 of 19544\n",
      "\tPreprocessing: 1250 of 19544\n",
      "\tPreprocessing: 1500 of 19544\n",
      "\tPreprocessing: 1750 of 19544\n",
      "\tPreprocessing: 2000 of 19544\n",
      "\tPreprocessing: 2250 of 19544\n",
      "\tPreprocessing: 2500 of 19544\n",
      "\tPreprocessing: 2750 of 19544\n",
      "\tPreprocessing: 3000 of 19544\n",
      "\tPreprocessing: 3250 of 19544\n",
      "\tPreprocessing: 3500 of 19544\n",
      "\tPreprocessing: 3750 of 19544\n",
      "\tPreprocessing: 4000 of 19544\n",
      "\tPreprocessing: 4250 of 19544\n",
      "\tPreprocessing: 4500 of 19544\n",
      "\tPreprocessing: 4750 of 19544\n",
      "\tPreprocessing: 5000 of 19544\n",
      "\tPreprocessing: 5250 of 19544\n",
      "\tPreprocessing: 5500 of 19544\n",
      "\tPreprocessing: 6000 of 19544\n",
      "\tPreprocessing: 6250 of 19544\n",
      "\tPreprocessing: 6500 of 19544\n",
      "\tPreprocessing: 6750 of 19544\n",
      "\tPreprocessing: 7000 of 19544\n",
      "\tPreprocessing: 7250 of 19544\n",
      "\tPreprocessing: 7500 of 19544\n",
      "\tPreprocessing: 7750 of 19544\n",
      "\tPreprocessing: 8000 of 19544\n",
      "\tPreprocessing: 8250 of 19544\n",
      "\tPreprocessing: 8500 of 19544\n",
      "\tPreprocessing: 8750 of 19544\n",
      "\tPreprocessing: 9000 of 19544\n",
      "\tPreprocessing: 9250 of 19544\n",
      "\tPreprocessing: 9500 of 19544\n",
      "\tPreprocessing: 9750 of 19544\n",
      "\tPreprocessing: 10000 of 19544\n",
      "\tPreprocessing: 10250 of 19544\n",
      "\tPreprocessing: 10500 of 19544\n",
      "\tPreprocessing: 10750 of 19544\n",
      "\tPreprocessing: 11000 of 19544\n",
      "\tPreprocessing: 11250 of 19544\n",
      "\tPreprocessing: 11500 of 19544\n",
      "\tPreprocessing: 11750 of 19544\n",
      "\tPreprocessing: 12000 of 19544\n",
      "\tPreprocessing: 12250 of 19544\n",
      "\tPreprocessing: 12500 of 19544\n",
      "\tPreprocessing: 12750 of 19544\n",
      "\tPreprocessing: 13000 of 19544\n",
      "\tPreprocessing: 13250 of 19544\n",
      "\tPreprocessing: 13500 of 19544\n",
      "\tPreprocessing: 13750 of 19544\n",
      "\tPreprocessing: 14000 of 19544\n",
      "\tPreprocessing: 14250 of 19544\n",
      "\tPreprocessing: 14500 of 19544\n",
      "\tPreprocessing: 14750 of 19544\n",
      "\tPreprocessing: 15000 of 19544\n",
      "\tPreprocessing: 15250 of 19544\n",
      "\tPreprocessing: 15500 of 19544\n",
      "\tPreprocessing: 15750 of 19544\n",
      "\tPreprocessing: 16000 of 19544\n",
      "\tPreprocessing: 16250 of 19544\n",
      "\tPreprocessing: 16500 of 19544\n",
      "\tPreprocessing: 16750 of 19544\n",
      "\tPreprocessing: 17000 of 19544\n",
      "\tPreprocessing: 17250 of 19544\n",
      "\tPreprocessing: 17500 of 19544\n",
      "\tPreprocessing: 17750 of 19544\n",
      "\tPreprocessing: 18000 of 19544\n",
      "\tPreprocessing: 18250 of 19544\n",
      "\tPreprocessing: 18500 of 19544\n",
      "\tPreprocessing: 18750 of 19544\n",
      "\tPreprocessing: 19000 of 19544\n",
      "\tPreprocessing: 19250 of 19544\n",
      "\tPreprocessing: 19500 of 19544\n",
      "\t ... done. Constructed word matrix of size: (19258, 300)\n",
      "Now computing similarities!\n",
      "Similarity : 0 of 183870\n",
      "Similarity : 100 of 183870\n",
      "Similarity : 200 of 183870\n",
      "Similarity : 300 of 183870\n",
      "Similarity : 400 of 183870\n",
      "Similarity : 500 of 183870\n",
      "Similarity : 600 of 183870\n",
      "Similarity : 700 of 183870\n",
      "Similarity : 800 of 183870\n",
      "Similarity : 900 of 183870\n",
      "Similarity : 1000 of 183870\n",
      "Similarity : 1100 of 183870\n",
      "Similarity : 1200 of 183870\n",
      "Similarity : 1300 of 183870\n",
      "Similarity : 1400 of 183870\n",
      "Similarity : 1500 of 183870\n",
      "Similarity : 1600 of 183870\n",
      "Similarity : 1700 of 183870\n",
      "Similarity : 1800 of 183870\n",
      "Similarity : 1900 of 183870\n",
      "Similarity : 2000 of 183870\n",
      "Similarity : 2100 of 183870\n",
      "Similarity : 2200 of 183870\n",
      "Similarity : 2300 of 183870\n",
      "Similarity : 2400 of 183870\n",
      "Similarity : 2500 of 183870\n",
      "Similarity : 2600 of 183870\n",
      "Similarity : 2700 of 183870\n",
      "Similarity : 2800 of 183870\n",
      "Similarity : 2900 of 183870\n",
      "Similarity : 3000 of 183870\n",
      "Similarity : 3100 of 183870\n",
      "Similarity : 3200 of 183870\n",
      "Similarity : 3300 of 183870\n",
      "Similarity : 3400 of 183870\n",
      "Similarity : 3500 of 183870\n",
      "Similarity : 3600 of 183870\n",
      "Similarity : 3700 of 183870\n"
     ]
    }
   ],
   "source": [
    "def compute_wv(model, a, a_star, b):\n",
    "    if a not in model.word_index or a_star not in model.word_index or b not in model.word_index:\n",
    "        return None\n",
    "    a, a_star, b = model[a], model[a_star], model[b]\n",
    "    v = a_star - a\n",
    "    b_star = b + v\n",
    "    return b_star\n",
    "\n",
    "def compute_scores(data, model, task_labels):\n",
    "    correct = 0\n",
    "    reciprocal_ranks = []\n",
    "    skipped = 0\n",
    "    word_vectors = []\n",
    "    parsed_data = []\n",
    "    for index, (a, a_star, b, b_star_actual) in enumerate(data):\n",
    "        # if the final word doesn't exist, then there's no point\n",
    "        if b_star_actual not in model.word_index:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        if index % 250 == 0:\n",
    "            print(\"\\tPreprocessing: {} of {}\".format(index, len(data)))\n",
    "        \n",
    "        b_star = compute_wv(model, a, a_star, b)\n",
    "        if b_star is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        parsed_data.append((a, a_star, b, b_star_actual))\n",
    "        word_vectors.append(b_star)\n",
    "\n",
    "    word_vectors = np.array(word_vectors)\n",
    "    print(\"\\t ... done. Constructed word matrix of size: {}\".format(word_vectors.shape))\n",
    "    print(\"Now computing similarities!\")\n",
    "    \n",
    "    similarity_dict = defaultdict(list)\n",
    "    for index, (word, word_index) in enumerate(model.word_index.items()):\n",
    "        if index % 100 == 0:\n",
    "            print(\"Similarity : {} of {}\".format(index, len(model.word_index)))\n",
    "        word_vector = model.embeddings[word_index]\n",
    "        \n",
    "        similarities = np.apply_along_axis(lambda _: cosine_similarity(word_vector, _), 1, word_vectors)\n",
    "        \n",
    "        for idx, tup in enumerate(parsed_data):\n",
    "            similarity_dict[tuple(tup)].append((word, similarities[idx]))\n",
    "    \n",
    "    correct = defaultdict(list)\n",
    "    reciprocal_ranks = defaultdict(list)\n",
    "    \n",
    "    for tup, task in zip(data, task_labels):\n",
    "        tup = tuple(tup)\n",
    "        if tup not in similarity_dict:\n",
    "            continue\n",
    "        similarity_list = similarity_dict[tup]\n",
    "        similarity_list.sort(key=lambda _: -_[1])\n",
    "        if similarity_list[0] == b_star_actual:\n",
    "            correct[task].append(1)\n",
    "        else:\n",
    "            correct[task].append(0)\n",
    "        b_star_results = [_[0] for _ in similarity_list]\n",
    "        reciprocal_ranks[task].append(reciprocal_rank(b_star_actual, b_star_results))\n",
    "    \n",
    "    overall_correct = []\n",
    "    overall_reciprocal_ranks = []\n",
    "    for task in correct.keys():\n",
    "        overall_correct.extend(correct[task])\n",
    "        overall_reciprocal_ranks.extend(reciprocal_ranks[task])\n",
    "        \n",
    "        accuracy = np.sum(correct[task]) / len(correct[task])\n",
    "        mrr = np.mean(reciprocal_ranks[task])\n",
    "        print(\"Task: {} :: Accuracy: {}, MRR: {}\".format(task, accuracy, mrr))\n",
    "    \n",
    "    overall_acc = np.sum(overall_correct) / len(overall_correct) \n",
    "    print(\"Overall: Accuracy: {}, MRR: {}\".format(overall_acc, np.mean(overall_reciprocal_ranks)))\n",
    "    \n",
    "    print(\"\\t ... done\")\n",
    "    print(\"\\t Skipped: {}\".format(skipped))\n",
    "\n",
    "compute_scores(analogy_data, bow2_sim, task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
