{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentEval usage example\n",
    "\n",
    "* Clone repo from FAIR github\n",
    "```\n",
    "    git clone https://github.com/facebookresearch/SentEval.git\n",
    "    cd SentEval/\n",
    "```\n",
    "* Dependencies:\n",
    "    * Python 2/3 with NumPy/SciPy\n",
    "    * Pytorch\n",
    "    * scikit-learn>=0.18.0\n",
    "\n",
    "* Install senteval\n",
    "```\n",
    "    python setup.py install\n",
    "```\n",
    "* Download datasets (it takes some time...)\n",
    "    * these are downstream tasks\n",
    "    * new Senteval also has probing tasks (https://github.com/facebookresearch/SentEval/tree/master/data/probing) for evaluating linguistic properties of your embeddings. \n",
    "```\n",
    "    cd data/downstream/\n",
    "    ./get_transfer_data.bash\n",
    "```\n",
    "* Download pretained Glove embeddings:\n",
    "\n",
    "```\n",
    "    mkdir pretrained\n",
    "    cd pretrained\n",
    "    wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "   \n",
    "```\n",
    "\n",
    "* The following code evaluates Glove pretrained embeddings on different NLP downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Create dictionary\n",
    "def create_dictionary(sentences, threshold=0):\n",
    "    words = {}\n",
    "    for s in sentences:\n",
    "        for word in s:\n",
    "            words[word] = words.get(word, 0) + 1\n",
    "\n",
    "    if threshold > 0:\n",
    "        newwords = {}\n",
    "        for word in words:\n",
    "            if words[word] >= threshold:\n",
    "                newwords[word] = words[word]\n",
    "        words = newwords\n",
    "    words['<s>'] = 1e9 + 4\n",
    "    words['</s>'] = 1e9 + 3\n",
    "    words['<p>'] = 1e9 + 2\n",
    "\n",
    "    sorted_words = sorted(words.items(), key=lambda x: -x[1])  # inverse sort\n",
    "    id2word = []\n",
    "    word2id = {}\n",
    "    for i, (w, _) in enumerate(sorted_words):\n",
    "        id2word.append(w)\n",
    "        word2id[w] = i\n",
    "\n",
    "    return id2word, word2id\n",
    "\n",
    "\n",
    "# Get word vectors from vocabulary (glove, word2vec, fasttext ..)\n",
    "def get_wordvec(path_to_vec, word2id):\n",
    "    word_vec = {}\n",
    "\n",
    "    with io.open(path_to_vec, 'r', encoding='utf-8') as f:\n",
    "        # if word2vec or fasttext file : skip first line \"next(f)\"\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word2id:\n",
    "                word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "    logging.info('Found {0} words with word vectors, out of \\\n",
    "        {1} words'.format(len(word_vec), len(word2id)))\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 13:28:34,219 : ***** Transfer task : MR *****\n",
      "\n",
      "\n",
      "2018-05-21 13:29:54,241 : Found 18490 words with word vectors, out of         20328 words\n",
      "2018-05-21 13:29:54,278 : Generating sentence embeddings\n",
      "2018-05-21 13:29:55,557 : Generated sentence embeddings\n",
      "2018-05-21 13:29:55,560 : Training sklearn-LogReg with (inner) 10-fold cross-validation\n",
      "2018-05-21 13:31:01,937 : Best param found at split 1: l2reg = 1                 with score 78.15\n",
      "2018-05-21 13:32:08,339 : Best param found at split 2: l2reg = 2                 with score 78.04\n",
      "2018-05-21 13:33:15,008 : Best param found at split 3: l2reg = 1                 with score 78.01\n",
      "2018-05-21 13:34:20,844 : Best param found at split 4: l2reg = 1                 with score 77.96\n",
      "2018-05-21 13:35:26,332 : Best param found at split 5: l2reg = 2                 with score 78.18\n",
      "2018-05-21 13:36:33,329 : Best param found at split 6: l2reg = 4                 with score 77.9\n",
      "2018-05-21 13:37:40,443 : Best param found at split 7: l2reg = 4                 with score 77.94\n",
      "2018-05-21 13:38:46,648 : Best param found at split 8: l2reg = 1                 with score 77.88\n",
      "2018-05-21 13:39:31,691 : Best param found at split 9: l2reg = 1                 with score 78.13\n",
      "2018-05-21 13:40:07,291 : Best param found at split 10: l2reg = 2                 with score 77.94\n",
      "2018-05-21 13:40:08,028 : Dev acc : 78.01 Test acc : 78.05\n",
      "\n",
      "2018-05-21 13:40:08,036 : ***** Transfer task : CR *****\n",
      "\n",
      "\n",
      "2018-05-21 13:41:26,865 : Found 5477 words with word vectors, out of         5677 words\n",
      "2018-05-21 13:41:26,886 : Generating sentence embeddings\n",
      "2018-05-21 13:41:27,076 : Generated sentence embeddings\n",
      "2018-05-21 13:41:27,077 : Training sklearn-LogReg with (inner) 10-fold cross-validation\n",
      "2018-05-21 13:41:36,653 : Best param found at split 1: l2reg = 2                 with score 80.4\n",
      "2018-05-21 13:41:47,157 : Best param found at split 2: l2reg = 1                 with score 79.6\n",
      "2018-05-21 13:41:59,454 : Best param found at split 3: l2reg = 4                 with score 80.28\n",
      "2018-05-21 13:42:11,403 : Best param found at split 4: l2reg = 2                 with score 80.1\n",
      "2018-05-21 13:42:23,440 : Best param found at split 5: l2reg = 2                 with score 80.6\n",
      "2018-05-21 13:42:35,365 : Best param found at split 6: l2reg = 1                 with score 81.1\n",
      "2018-05-21 13:42:47,344 : Best param found at split 7: l2reg = 4                 with score 80.25\n",
      "2018-05-21 13:42:59,386 : Best param found at split 8: l2reg = 2                 with score 79.99\n",
      "2018-05-21 13:43:11,574 : Best param found at split 9: l2reg = 1                 with score 80.11\n",
      "2018-05-21 13:43:23,620 : Best param found at split 10: l2reg = 4                 with score 80.46\n",
      "2018-05-21 13:43:23,921 : Dev acc : 80.29 Test acc : 79.63\n",
      "\n",
      "2018-05-21 13:43:23,925 : ***** Transfer task : MPQA *****\n",
      "\n",
      "\n",
      "2018-05-21 13:44:42,877 : Found 6202 words with word vectors, out of         6241 words\n",
      "2018-05-21 13:44:42,909 : Generating sentence embeddings\n",
      "2018-05-21 13:44:43,142 : Generated sentence embeddings\n",
      "2018-05-21 13:44:43,143 : Training sklearn-LogReg with (inner) 10-fold cross-validation\n",
      "2018-05-21 13:45:19,195 : Best param found at split 1: l2reg = 0.5                 with score 87.71\n",
      "2018-05-21 13:45:57,428 : Best param found at split 2: l2reg = 0.25                 with score 87.92\n",
      "2018-05-21 13:46:36,683 : Best param found at split 3: l2reg = 0.25                 with score 87.81\n",
      "2018-05-21 13:47:15,545 : Best param found at split 4: l2reg = 0.25                 with score 88.13\n",
      "2018-05-21 13:47:56,511 : Best param found at split 5: l2reg = 0.25                 with score 87.71\n",
      "2018-05-21 13:48:36,896 : Best param found at split 6: l2reg = 0.5                 with score 87.82\n",
      "2018-05-21 13:49:17,527 : Best param found at split 7: l2reg = 0.25                 with score 87.76\n",
      "2018-05-21 13:49:58,708 : Best param found at split 8: l2reg = 0.5                 with score 88.04\n",
      "2018-05-21 13:50:38,921 : Best param found at split 9: l2reg = 0.25                 with score 87.75\n",
      "2018-05-21 13:51:17,209 : Best param found at split 10: l2reg = 0.25                 with score 87.57\n",
      "2018-05-21 13:51:17,796 : Dev acc : 87.82 Test acc : 88.0\n",
      "\n",
      "2018-05-21 13:51:17,809 : ***** Transfer task : SUBJ *****\n",
      "\n",
      "\n",
      "2018-05-21 13:52:36,339 : Found 20798 words with word vectors, out of         22639 words\n",
      "2018-05-21 13:52:36,365 : Generating sentence embeddings\n",
      "2018-05-21 13:52:36,991 : Generated sentence embeddings\n",
      "2018-05-21 13:52:36,993 : Training sklearn-LogReg with (inner) 10-fold cross-validation\n",
      "2018-05-21 13:53:03,142 : Best param found at split 1: l2reg = 4                 with score 91.7\n",
      "2018-05-21 13:53:31,985 : Best param found at split 2: l2reg = 1                 with score 91.74\n",
      "2018-05-21 13:54:00,698 : Best param found at split 3: l2reg = 8                 with score 91.88\n",
      "2018-05-21 13:54:29,758 : Best param found at split 4: l2reg = 2                 with score 92.04\n",
      "2018-05-21 13:54:58,610 : Best param found at split 5: l2reg = 2                 with score 91.64\n",
      "2018-05-21 13:55:27,417 : Best param found at split 6: l2reg = 1                 with score 91.61\n",
      "2018-05-21 13:55:55,978 : Best param found at split 7: l2reg = 2                 with score 91.77\n",
      "2018-05-21 13:56:24,646 : Best param found at split 8: l2reg = 1                 with score 91.99\n",
      "2018-05-21 13:56:53,588 : Best param found at split 9: l2reg = 8                 with score 91.77\n",
      "2018-05-21 13:57:22,727 : Best param found at split 10: l2reg = 2                 with score 91.53\n",
      "2018-05-21 13:57:23,245 : Dev acc : 91.77 Test acc : 91.69\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SUBJ': {'acc': 91.69, 'devacc': 91.77, 'ndev': 10000, 'ntest': 10000}}\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import sklearn\n",
    "\n",
    "# Set PATHs\n",
    "# path to senteval\n",
    "PATH_TO_SENTEVAL = 'SentEval'\n",
    "# path to the NLP datasets \n",
    "PATH_TO_DATA = 'SentEval/data/'\n",
    "# path to glove embeddings\n",
    "PATH_TO_VEC = 'SentEval/data/downstream/pretrained/glove.840B.300d.txt'\n",
    "\n",
    "\n",
    "# import SentEval\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "import senteval\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    \"\"\"\n",
    "    In this example we are going to load Glove, \n",
    "    here you will initialize your model.\n",
    "    remember to add what you model needs into the params dictionary\n",
    "    \"\"\"\n",
    "    _, params.word2id = create_dictionary(samples)\n",
    "    # load glove/word2vec format \n",
    "    params.word_vec = get_wordvec(PATH_TO_VEC, params.word2id)\n",
    "    # dimensionality of glove embeddings\n",
    "    params.wvec_dim = 300\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    \"\"\"\n",
    "    In this example we use the average of word embeddings as a sentence representation.\n",
    "    Each batch consists of one vector for sentence.\n",
    "    Here you can process each sentence of the batch, \n",
    "    or a complete batch (you may need masking for that).\n",
    "    \n",
    "    \"\"\"\n",
    "    # if a sentence is empty dot is set to be the only token\n",
    "    # you can change it into NULL dependening in your model\n",
    "    batch = [sent if sent != [] else ['.'] for sent in batch]\n",
    "    embeddings = []\n",
    "\n",
    "    for sent in batch:\n",
    "        sentvec = []\n",
    "        # the format of a sentence is a lists of words (tokenized and lowercased)\n",
    "        for word in sent:\n",
    "            if word in params.word_vec:\n",
    "                # [number of words, embedding dimensionality]\n",
    "                sentvec.append(params.word_vec[word])\n",
    "        if not sentvec:\n",
    "            vec = np.zeros(params.wvec_dim)\n",
    "            # [number of words, embedding dimensionality]\n",
    "            sentvec.append(vec)\n",
    "        # average of word embeddings for sentence representation\n",
    "        # [embedding dimansionality]\n",
    "        sentvec = np.mean(sentvec, 0)\n",
    "        embeddings.append(sentvec)\n",
    "    # [batch size, embedding dimensionality]\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Set params for SentEval\n",
    "# we use logistic regression (usepytorch: Fasle) and kfold 10\n",
    "# In this dictionary you can add extra information that you model needs for initialization\n",
    "# for example the path to a dictionary of indices, of hyper parameters\n",
    "# this dictionary is passed to the batched and the prepare fucntions\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': False, 'kfold': 10}\n",
    "# this is the config for the NN classifier but we are going to use scikit-learn logistic regression with 10 kfold\n",
    "# usepytorch = False \n",
    "#params_senteval['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "#                                 'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    \n",
    "    # here you define the NLP taks that your embedding model is going to be evaluated\n",
    "    # in (https://arxiv.org/abs/1802.05883) we use the following :\n",
    "    # SICKRelatedness (Sick-R) needs torch cuda to work (even when using logistic regression), \n",
    "    # but STS14 (semantic textual similarity) is a similar type of semantic task\n",
    "    transfer_tasks = ['MR', 'CR', 'MPQA', 'SUBJ']\n",
    "    for task in transfer_tasks:\n",
    "        # senteval prints the results and returns a dictionary with the scores\n",
    "        try:\n",
    "            results = se.eval([task])\n",
    "        except:\n",
    "            print(\"Failed: \", task)\n",
    "            continue\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
